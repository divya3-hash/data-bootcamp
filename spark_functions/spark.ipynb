{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db16c29d-3b33-4555-8c32-c51da058bab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c91dc2-d059-46bf-be81-889e4b3906b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\",   \"IT\",       60000,   \"Bengaluru\"),\n",
    "    (2, \"Bob\",     \"HR\",       None,    \"Mumbai\"),\n",
    "    (3, \"Charlie\", None,       55000,   None),\n",
    "    (4, \"David\",   \"Finance\",  70000,   \"Delhi\"),\n",
    "    (5, None,      \"IT\",       50000,   None),\n",
    "    (6, \"Eva\",     \"Marketing\",None,    \"Chennai\"),\n",
    "    (7, \"Frank\",   None,       65000,   \"Bengaluru\"),\n",
    "    (8, \"Grace\",   \"Finance\",  80000,   None)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"employee_id\", \"name\", \"department\", \"salary\", \"city\"]\n",
    "\n",
    "#creating dataFrame\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9120ffee-acd2-4157-8d5e-613f50ea72f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#Filtering out the nulls using \"filter\"\n",
    "filter_nulls_dpt=df.filter(col(\"department\").isNotNull())\n",
    "filter_nulls_dpt.show()\n",
    "\n",
    "data_fillna_dpt=df.fillna({\"department\":\"default_NA\"})  \n",
    "data_fillna_dpt.show()\n",
    "\n",
    "data_coalese_nulls_dpt=df.withColumn(\"department\",coalesce(col(\"department\"),lit(\"default_NA\")))\n",
    "data_coalese_nulls_dpt.show()\n",
    "\n",
    "data_with_nulls_dpt=df.withColumn(\"department\",when(col(\"department\").isNull(),\"UNKNOWN\").otherwise(col(\"department\"))) \n",
    "data_with_nulls_dpt.show()                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1c90b3-2daa-4894-8082-bca7cf2dd9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " from pyspark.sql.functions import *\n",
    " MEAN_VALUES=df.select(mean(col(\"salary\"))).first()[0]\n",
    " mean_df_nulls=df.withColumn(\"salary\",when(col(\"salary\").isNull(),MEAN_VALUES).otherwise(col(\"salary\")))\n",
    " mean_df_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e3579f-46b6-41b4-945d-5724d24e8722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1=sc.parallise([1,2,3,4,5])\n",
    "map_rdd=rdd1.map(lambda x:x**2)\n",
    "map_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc70a86b-85d7-4408-87b7-5ad6351cdbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1=spark.createDataFrame([1,2,3,4,5],\"value\")\n",
    "map_rdd=rdd1.map(lambda x:x**2)\n",
    "map_rdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d19147-e809-49a2-8b37-188127b6839c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[\n",
    "    (1,\"ali\"),\n",
    "    (2,\"bob\"),\n",
    "    (3,\"charlie\"),\n",
    "    (4,\"david\")\n",
    "    ]\n",
    "schema=[\"id\",\"name\"]\n",
    "\n",
    "df_student=spark.createDataFrame(data,schema)\n",
    "df_student.show()\n",
    "\n",
    "data_2=[1,3]\n",
    "schema_2=[\"id\"]\n",
    "df_student_2=spark.createDataFrame(data_2,schema_2)\n",
    "df_student_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f50db4-98f9-46e0-b8f3-dea92b9ac85f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_student.join(df_student_2,df_student.id==df_student_2.id,\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5e49ca-ab7d-4eac-83c3-7f4cc64db848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SampleData\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\"),\n",
    "    (2, \"Bob\", \"Finance\"),\n",
    "    (3, \"Charlie\", \"IT\"),\n",
    "    (4, \"David\", \"Marketing\"),\n",
    "    (5, \"Emma\", \"Operations\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88813b0d-a866-45ca-8505-081e3296720d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.groupBy(\"department\")\\\n",
    "\t.agg(count(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d037514-569e-40e0-89e1-98033f2c1f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "windows=Window.partitionBy(\"department\").orderBy(\"id\")\n",
    "win_df=df.withColumn(\"new_clm\",count(\"id\").over(windows))\n",
    "win_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
