{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0899ab89-41e0-4a83-9185-5820fc2e847b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#python pgm to find the frequencies of a given text/ string\n",
    "\n",
    "def freq_text(text):\n",
    "    freq={}\n",
    "    for i in text:\n",
    "        if i in freq:\n",
    "              freq[i]+=1 #adding counter\n",
    "        else:\n",
    "            freq[i]=1 #initializing counter\n",
    "    return freq  \n",
    "\n",
    "text=\"abvdjhweh\"\n",
    "res=(freq_text(text))\n",
    "print(res)\n",
    "print(type(res))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d6fc4d3-6416-4277-948b-e5d4109850ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "\n",
    "# For each customer_id, calculate the difference of total_amount between the current and previous order\n",
    "from pyspark.sql.functions import col,lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"c1\", \"2025-01-05\", 150),\n",
    "    (\"c1\", \"2025-01-01\", 100),\n",
    "    (\"c1\", \"2025-01-10\", 120),\n",
    "    (\"c2\", \"2025-01-02\", 200),\n",
    "    (\"c2\", \"2025-01-07\", 180),\n",
    "    (\"c3\", \"2025-01-12\", 90),\n",
    "    (\"c3\", \"2025-01-03\", 50),\n",
    "    (\"c3\", \"2025-01-08\", 70),\n",
    "]\n",
    "column=[\"customer_id\",\"start_date\",\"total_amt\"]\n",
    "df_data=spark.createDataFrame(data,column)\n",
    "#display(df_data)\n",
    "df_window=Window.partitionBy(\"customer_id\").orderBy(\"start_date\")\n",
    "df_diff=(df_data.withColumn(\"diff_col\",lag(\"total_amt\").over(df_window))\n",
    "         .withColumn(\"diff_col\",col(\"total_amt\")-col(\"diff_col\")))\n",
    "display(df_diff)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841af3d6-1131-47a9-88a5-0d0c20341999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "--sql\n",
    "Table1   Table2\n",
    "1  \t \t\t2 \n",
    "2\t \t\t2\n",
    "2\t \t\t3\n",
    "NULL\t NULL\n",
    "         NULL\n",
    " \n",
    "INNER JOIN, LEFT JOIN, RIGHT JOIN, CROSS JOIN output row counts..?\n",
    " inner join : 0+(2*2)=4 \n",
    " left : 4+2=6\n",
    " right : 4 + 3 =7\n",
    " cross : 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b372ee7a-790b-4521-bbc1-bf4c43ff3ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--pyspark senario based\n",
    " >source - 100 gb (hadoop,S3) : present in s3\n",
    " >question is when u load this in parquet file in s3, how many files it will make.\n",
    "spark code\n",
    "line 1 - read the csv file from source\n",
    "line 2 - write to output folder in parquet format\n",
    "\n",
    "sol:\n",
    "100 gb * 1024 mb / 128 mb for each = 800.parquet files\n",
    "df.groupby(\"col1\") --> groupby does default partitions :200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f38f02b-dc27-41c9-9e09-ebb1e8b59d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "pyspark :\n",
    "\n",
    "source\n",
    "age - num(data type)\n",
    "after 2 months, in the \"age\" field ->string values are coming , how will u handle it?\n",
    "\n",
    "sol: while reafing the data , dont use 'infershema' use / create a schema naturally\n",
    "\n",
    "like shown below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce913070-c6fd-4944-9415-483f32ffcd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)   # force numeric type\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"/path/to/file.csv\", schema=schema, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3645f8-2c46-475a-9f4e-553104c3a528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(tell which is job,stage,tasks) in given\n",
    "Read csv file\n",
    "show() \n",
    "groupBy()  \n",
    "filter() \n",
    "repartition(100) \n",
    "join() \n",
    "filter()\n",
    "write to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a35b8035-e609-4644-8f8a-1f0790e120ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "sol :\n",
    "Read CSV (lazy transformation)\n",
    "        |\n",
    "        v\n",
    "show()  ---> Action → Job 1\n",
    "        |\n",
    "        v\n",
    "Stage 1: Scan CSV partitions → Tasks = #input partitions\n",
    "--------------------------------------------------------\n",
    "\n",
    "groupBy() ---> Wide transformation → Shuffle → Stage 2\n",
    "        |\n",
    "        v\n",
    "filter() ---> Narrow transformation → stays in Stage 2\n",
    "        |\n",
    "        v\n",
    "repartition(100) ---> Wide transformation → Shuffle → Stage 3\n",
    "        |\n",
    "        v\n",
    "join() ---> Wide transformation → Shuffle → Stage 4\n",
    "        |\n",
    "        v\n",
    "filter() ---> Narrow transformation → stays in Stage 4\n",
    "        |\n",
    "        v\n",
    "write() ---> Action → Job 2\n",
    "        |\n",
    "        v\n",
    "Stage 5+: Shuffle + Output write → Tasks = #output partitions\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-26 21_22_04",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
