{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2771cf3-d3c8-48b4-a188-156592a6bc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- You are working with a financial services company that processes transactions data. \n",
    "- Each transaction contains a JSON field with transaction details,which is stored as a string.\n",
    "- You need to update the schema to handle these JSON fields properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba9d98d-c31e-45c0-a5cc-6e4f7e9c22e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Step 1: Raw DataFrame\n",
    "#Letâ€™s say you start with a table or CSV like this:\n",
    "#from pyspark.sql import SparkSession (no need pf importing in databricks, has bydefault)\n",
    "\n",
    "dataa = [\n",
    "    (1001, \"2025-09-18 10:00:00\", '{\"amount\": 250.75, \"currency\": \"INR\", \"merchant\": \"Flipkart\", \"status\": \"success\"}'),\n",
    "    (1002, \"2025-09-18 10:05:00\", '{\"amount\": 1200.00, \"currency\": \"USD\", \"merchant\": \"Amazon\", \"status\": \"failed\"}')\n",
    "]\n",
    "columns = [\"transaction_id\", \"timestamp\", \"transaction_details\"]\n",
    "df_raw = spark.createDataFrame(dataa, columns)\n",
    "display(df_raw)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469fe313-6bbf-488a-bef6-a6ce4d0ff283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#parse JSON column\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"currency\", StringType()),\n",
    "    StructField(\"merchant\", StringType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])\n",
    "\n",
    "df_parsed = df_raw.withColumn(\"details\", from_json(col(\"transaction_details\"), json_schema))\n",
    "display(df_parsed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db7ede9-7b13-4f79-a2f4-35774887952a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Step 3: Flatten the Structure\n",
    "df_final = df_parsed.select(\n",
    "    \"transaction_id\",\n",
    "    \"timestamp\",\n",
    "    col(\"details.amount\").alias(\"amount\"),\n",
    "    col(\"details.currency\").alias(\"currency\"),\n",
    "    col(\"details.merchant\").alias(\"merchant\"),\n",
    "    col(\"details.status\").alias(\"status\")\n",
    ")\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95e527b-bfd4-4133-9ef7-ec1a06080d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#create a DF with column \"add\" and values in the column are 01,01,02 (now dilter the value \"01\")\n",
    "\n",
    "df=spark.createDataFrame(['01','01','02'],['add'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fd24d7-9293-453e-a8c5-5f1130b89bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_new=df.filter(col(\"add\")=='01')\n",
    "display(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e31a7a-c6d6-413d-868c-db73e0255497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#DROP a column\n",
    "\n",
    "df_drop_column=df.drop(\"add\")\n",
    "display(df_drop_column)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "interview questions",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
